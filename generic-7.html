<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Generic - Stellar by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<h1>Stress-Testing LLMs on Legal Compression</h1>
						<p>Blog</p>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Content -->
							<section id="content" class="main">
								<h2>Introduction</h2>
								<p> Indian Court judgments are famously long, intricate, and packed with layered reasoning. For anyone working with public policy, law, or research, turning these massive texts into clear, reliable summaries is almost a project by itself. Thus I'm here trying to test whether modern language models can summarise Indian legal judgments with accuracy, structure, and speed.

To explore this, I put three very different models to work — Google Gemini, Phi-4 Mini (running locally through Ollama), and Facebook BART. Each comes with its own strengths, quirks, and limits, making them ideal for a comparative test. My goal wasn’t to chase perfection, but to understand how well these models handle long-form judicial reasoning, how they behave under load, and where they start to struggle.

This blog documents that journey — from extracting text to chunking, prompting, debugging, and watching the models do their best (and sometimes fail spectacularly) to summarise some of India’s most complex constitutional judgments. It’s a practical look at what really happens when AI meets legal analysis, and what we can learn from the process. </p>		 	                            

								<h3> Why Legal Judgments Are the Final Boss of NLP</h3>
								<p> 

The nature of Indian Supreme Court judgments: long, dense, layered with facts, arguments, citations, and constitutional doctrine.

That is why summarising them is difficult: nuance, accuracy, logical continuity, and massive length.

The framework proposed in Evaluating Large Language Models by Irina Sigler and Yuan Xu offers a grounded way to think about how we measure an LLM’s real-world performance.

								<h3>Qwen 2.5</h3>
<li><b>Hugging Face API:</b> I first started with the Qwen 2.5 7B model on hugging face by plugging in the free HF API key - but, since the document I want to summarise is a 200 page pdf - the model calls hit the Hugging Face Inference API quota after summarising 34 out of the 240 chunks it created. So I went to the trusted ChatGPT to give me a workaround this.	</li>
<li><b>Local GPU:</b> The full PDF was summarised locally using a quantized Qwen 2.5 3B model with chunking and a final merge step. After running the final consolidation cell, it failed with a CUDA OutOfMemoryError because the model was asked to attend over all chunk summaries at once, exceeding GPU memory limits. This was a fundamental transformer limitation that comes with running the model locally. The fix was to redesign the pipeline into a hierarchical summarisation approach, first merging small groups of summaries and then generating a final structured brief from those intermediate summaries. This resolved the memory issue while preserving accuracy and scalability.</li>
<li><b>End Result:</b> The model showed strong capability in summarising long legal judgments by consistently extracting the Background & Facts, Core Legal Issues, Court’s Reasoning, and Final Verdict. It preserved the overall structure and logical flow of the judgment, indicating more than surface-level text understanding. While some nuanced legal reasoning was compressed, the essential arguments and outcomes remained intact. This makes the model effective for rapid legal comprehension and large-scale analytical use, with scope for further refinement and scalability.</li>									
<p> You won’t get a meaningful single-prompt “upload 350k words and get back a perfect one-pass summary” for free on Colab — the engineering pattern (chunk + embed + summarize + reduce) is the proven, production approach. Use a long-context model where available to reduce engineering complexity, but if you want to stay fully on free/open infrastructure, quantized 7B models + hierarchical summarization will do the job well. </p>
                                <h3>Mistral 7.3B </h3>
								<ul class="actions special">
									<li><a href="https://www.kaggle.com/code/arushirawat/brent-oil-price-forecasting" class="button">Click to view the project</a></li>
								</ul>
							</section>

					</div>
